{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the GeoKB, we often need to build connections to USGS people from projects, publications, data assets, and other entities organized into the graph. Sticking with our approach of only dealing with public information sources, we source people records from the USGS Staff Profile listing along with clues about additional personnel that we pick up from the publication record (Pubs Warehouse, USGS Science Data Catalog, etc.). Not all of these sources agree with one another as they are all sourced from different internal data systems. However, these represent all of the people who have an essentially public record about themselved by virtue of either creating and maintaining a public profile page or publishing creative works of one kind or another under their name and identity.\n",
    "\n",
    "Working with the USGS web site is still kind of a pain in that we need to scrape HTML. This notebook builds on previous work under the iSAID project where we first worked through the scraping logic. Having the knowledgebase to work against presents a little bit of a different dynamic in how to operate this system. We treat entities that get created in the GeoKB as a starting point from which we can regularly go after additional information for claims. One source of instantiating person entities is the inventory of staff profiles that we have to run through and scrape via a paginated process. We also pick up clues about people that didn't come from that \"master\" source via Pubs Warehouse, Science Data Catalog, Model Catalog, and other sources of interest where we have authors/contributors listed. In GeoKB processing, we attempt to not introduce duplicate entities anywhere, so we try to build enough information about people with reasonably unique identifiers that we can use.\n",
    "\n",
    "In this exercise, I took a different tact on work I'd done previously in processing and caching the varous web scraping routines to less than optimally unstructured information from the web into structured data we can work against. Since we need to scrape different kinds of pages over time into different resulting data structures, including from changes in design that occur over time, I opted to start building out a running \"log\" of sorts. I continued to use the simplicity of a PostgreSQL/RDS table, because it is one of the simplest data store connections to deal with on CHS and it is plenty performant when operating from CHS Pangeo. (This notebook is set up to operate on a Python kernel running from the Pangeo environment.) The new \"isaid_web_scrape_cache\" table contains the URL scraped, a timestamp, the HTTP status code received, and a scrape structure stored as a JSON string.\n",
    "\n",
    "I tweaked the schema I had been using for personnel profiles a little bit here in the new function contained in this notebook. I've been working on some new methods for summarizing and extracting linkable entities from \"lumps of text\" (documents) using LLMs. From the profile pages, this mostly consists of what I put in here as \"body_html,\" essentially the full HTML content that some personnel have built out for their profiles. Along with the expertise terms and the new structures for education and professional experience, this is the unique content that is really only available from the Staff Profile pages themselves. The dynamic listings of publications, data, etc. included on the pages can all be built out from original catalog sources more efficiently or from other web scraping of \"science activities.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from wbmaker import WikibaseConnection\n",
    "import dateutil.parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "geokb = WikibaseConnection(\"GEOKB_CLOUD\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial starting point here is to pull all URLs associated with people that point to the \"staff-profiles\" path on the USGS web. These are the URLs we will be able to scrape with the process in the staff_profile_scrape() function. The following SPARQL query gets all \"instance of\" person who also have a reference URL, filtering to those URLs with \"staff-profiles\" in the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd_persons_w_profile_url = \"https://geokb.wikibase.cloud/query/sparql?query=PREFIX%20wd%3A%20%3Chttps%3A%2F%2Fgeokb.wikibase.cloud%2Fentity%2F%3E%0APREFIX%20wdt%3A%20%3Chttps%3A%2F%2Fgeokb.wikibase.cloud%2Fprop%2Fdirect%2F%3E%0A%0ASELECT%20%3Fperson%20%3FpersonLabel%20%3Furl%0AWHERE%20%7B%0A%20%20%3Fperson%20wdt%3AP1%20wd%3AQ3%20.%0A%20%20%3Fperson%20wdt%3AP31%20%3Furl%20.%0A%20%20FILTER(contains(str(%3Furl)%2C'staff-profiles'))%20.%0A%20%20SERVICE%20wikibase%3Alabel%20%7B%20bd%3AserviceParam%20wikibase%3Alanguage%20%22en%22%20.%20%7D%0A%7D\"\n",
    "\n",
    "df_wd_people = geokb.url_sparql_query(sparql_url=wd_persons_w_profile_url, output_format=\"dataframe\")\n",
    "df_wd_people[\"qid\"] = df_wd_people[\"person\"].apply(lambda x: x.split(\"/\")[-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This newish function for scraping USGS Staff Profile pages is not all that dissimilar to previous methods. The main thing I'm doing here is trying to soak up as much potentially useful content from the pages as possible without over-processing. I can then pull it out of the cache as an entire JSON document or query inside the JSON using postgres casting methods. Storing these documents in more of a running log form with a timestamp should serve our purposes well for anything we want to do with this information in future.\n",
    "\n",
    "From the GeoKB perspective, all I'm currently working up from this scrape is the ORCID identifier (not available publicly elsewhere). That gives us partial connection on authors/editors/contributors to the Pubs Warehouse. I'll revisit the long slate of \"expertise\" terms along with processing of text elements. Expertise terms are treated solely as tags from the USGS web perspective and do not link to any source of definition or semantics, so they will need some thought before introducing them into the knowledge graph. One approach would be to do what I did previously in iSAID, matching some terms to the USGS Thesaurus with a big assumption that those definitions would be agreed to by the person entering the terms. Another approach would be to initially dump everything into some kind of high-level classification as instances of a concept, link to them so they are in the graph, and then give them further definition and classification down the road somewhere. This is an overall philosophical approach we'll have to examine in many other cases as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "def staff_profile_scrape(profile_url):\n",
    "    profile = {\n",
    "        \"url\": profile_url,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"name\": None,\n",
    "        \"name_qualifier\": None,\n",
    "        \"title\": None,\n",
    "        \"organization_name\": None,\n",
    "        \"organization_link\": None,\n",
    "        \"email\": None,\n",
    "        \"orcid\": None,\n",
    "        \"intro_statements\": None,\n",
    "        \"expertise_terms\": None,\n",
    "        \"professional_experience\": None,\n",
    "        \"education\": None,\n",
    "        \"body_html\": None\n",
    "    }\n",
    "\n",
    "    r = requests.get(profile_url)\n",
    "    profile[\"status_code\"] = r.status_code\n",
    "\n",
    "    if r.status_code == 200:\n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "        first_h1 = soup.find('h1')\n",
    "        if first_h1:\n",
    "            name_text = unicodedata.normalize(\"NFKD\", first_h1.text.strip())\n",
    "            if name_text.endswith(\")\"):\n",
    "                name_qual = re.search(r'\\((.*?)\\)', name_text)\n",
    "                if name_qual:\n",
    "                    profile[\"name_qualifier\"] = name_qual.group(1)\n",
    "                    profile[\"name\"] = name_text.split(\"(\")[0].strip()\n",
    "                else:\n",
    "                    profile[\"name\"] = name_text\n",
    "            else:\n",
    "                profile[\"name\"] = name_text\n",
    "\n",
    "        org_div = soup.find('div', {'class': 'field-org-primary'})\n",
    "        if org_div:\n",
    "            microsite_div = org_div.find('div', {'class': 'field-microsite'})\n",
    "            if microsite_div:\n",
    "                org_link = microsite_div.find('a')\n",
    "                if org_link:\n",
    "                    profile[\"organization_name\"] = org_link.text.strip()\n",
    "                    profile[\"organization_link\"] = f\"https://www.usgs.gov{org_link['href']}\"\n",
    "\n",
    "            title_div = org_div.find('div', {'class': 'field-title'})\n",
    "            if title_div:\n",
    "                profile[\"title\"] = unicodedata.normalize(\"NFKD\", title_div.text.strip())\n",
    "\n",
    "        email_div = soup.find('div', {'class': 'field-email'})\n",
    "        if email_div:\n",
    "            profile[\"email\"] = email_div.text.strip()\n",
    "\n",
    "        orcid_div = soup.find('div', {'class': 'field--name--field-staff-orcid'})\n",
    "        if orcid_div:\n",
    "            profile[\"orcid\"] = orcid_div.text.strip()\n",
    "\n",
    "        intro_div = soup.find('div', {'class': 'field-intro'})\n",
    "        if intro_div:\n",
    "            profile[\"intro_statements\"] = [unicodedata.normalize(\"NFKD\", i.text.strip()) for i in intro_div.find_all('p')]\n",
    "\n",
    "        expertise_divs = soup.find_all('div', {'class': 'field-staff-expertise'})\n",
    "        if expertise_divs:\n",
    "            profile[\"expertise_terms\"] = [unicodedata.normalize(\"NFKD\", i.text.strip()) for i in expertise_divs]\n",
    "\n",
    "        professional_experience_items = soup.find_all('li', {'class': 'field-professional-experience'})\n",
    "        if professional_experience_items:\n",
    "            profile[\"professional_experience\"] = [unicodedata.normalize(\"NFKD\", i.text.strip()) for i in professional_experience_items]\n",
    "\n",
    "        education_items = soup.find_all('li', {'class': 'field-education'})\n",
    "        if education_items:\n",
    "            profile[\"education\"] = [unicodedata.normalize(\"NFKD\", i.text.strip()) for i in education_items]\n",
    "\n",
    "        body_div = soup.find('div', {'class': 'body'})\n",
    "        if body_div:\n",
    "            profile[\"body_html\"] = body_div.prettify()\n",
    "\n",
    "    return {\n",
    "                \"url\": profile[\"url\"],\n",
    "                \"timestamp\": profile[\"timestamp\"],\n",
    "                \"status_code\": profile[\"status_code\"],\n",
    "                \"scrape\": json.dumps(profile)\n",
    "            }\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need to run these through all that often. My initial cut here was to refresh my index of scraped profiles for every person entity I've confirmed and created in the GeoKB instance. I've also gone back through to rework the last available cache I had for people who are no longer showing up with a profile page (there is inconsistency in how these cases are handled) to put those into the same structure for historical record. I'll work up a concept of operations for this in future that uses some kind of refresh cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_profiles = Parallel(n_jobs=4)(delayed(staff_profile_scrape)(i) for i in tqdm(list(df_wd_people[\"url\"].unique())))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the one component that requires code to be run on an internal system at the moment. The RDS instance I'm using here is on an account that is not yet set up for any kind of access from outside the USGS network/CHS environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "526"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isaid = geokb.pg_cnxn(\n",
    "    db=\"isaid\",\n",
    "    db_user=os.environ[\"rds_username\"],\n",
    "    db_pass=os.environ[\"rds_password\"],\n",
    "    db_host=os.environ[\"rds_host\"],\n",
    "    db_port=os.environ[\"rds_port\"]\n",
    ")\n",
    "\n",
    "pd.DataFrame(scraped_profiles).to_sql(\n",
    "    name=\"isaid_web_scrape_cache\",\n",
    "    con=isaid,\n",
    "    index=False,\n",
    "    if_exists=\"replace\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:geokb]",
   "language": "python",
   "name": "conda-env-geokb-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
