{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a work in progress process for initializing the Geoscience Knowledgebase with a set of properties and foundational semantics that establishes a base to build from in curating geoscientific knowledge. Our initial use cases have to do with integrating mineral occurrence information along with document references contributing to those and other facts and concepts associated with conducting mineral resource assessment. In order to build a useful knowledge graph on these concepts, though, we also need to tie in lots of other things needed for the claims associated with these things to legitimately link to other things.\n",
    "\n",
    "For instance, we use NI \"43-101 Technical Reports\" and a newer \"SK-1300 Technical Report\" as sources for claims associated with mining projects/properties such as geographic location, mineral commodities identified and/or extracted, figures indicating estimates of ore grade and tonnage, and other details. These are technical geoscientific reports required by the Canadian and U.S. governments, respectively. We need an \"instance of\" (rdf:type) claim on everything like this in the system. While we could simply create items in the graph to represent these two classes with no further classification, it is useful to \"work back up the semantic hierarchy\" for as many concepts as we can as far as we need to in order for the information we are recording in the GeoKB to be understandable in the broader global knowledge commons (Wikidata and/or other efforts).\n",
    "\n",
    "The initialization process here is designed to give us a semantic baseline to work from as we pull in the information and connections we really care about within this context. We're taking a pragmatic approach that is slightly more rigorous (and certainly more streamlined) than the wild west of Wikidata but somewhere short of endlessly academic. We have to get a whole bunch of information into the GeoKB to support analytical use, so we make a best effort to align what we have with mature ontologies and namespaces, knowing we'll have to evolve it over time. The notebook approach on this gives us a good basis to record our reasoning and the places we have to make pragmatic tradeoffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pywikibot as pwb\n",
    "import json\n",
    "import pandas as pd\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from nested_lookup import nested_lookup\n",
    "from datetime import datetime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "If we want to run this and other notebooks as Lambdas eventually, we'll need to set up some paramters. I moved the stuff we want to keep somewhat secret to environment variables as a safety measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "sparql_endpoint = os.environ['SPARQL_ENDPOINT']\n",
    "wb_domain = os.environ['WB_DOMAIN']\n",
    "geokb_init_sheet_id = '1dbuKc4cZJz0YY81B2xWXM5fId6gWgzmQar3hg3CI0Rw'\n",
    "\n",
    "accepted_languages = ['en']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "All or most of these functions should be movable to the abstract Wikibase management python package we are designing. That needs to be applicable to the GeoKB but generic enough to apply in other types of domains and circumstances. There are other communities doing similar work such as the wikidataintegrator project in the health sciences. We just found a need to start from the basics of pywikibot and how it operates.\n",
    "\n",
    "I realize I'm introducing what could be higher overhead than necessary here with use of Pandas and other specialized packages. We can reevaluate that at the foundational level of the KB management package.\n",
    "\n",
    "### Label uniqueness\n",
    "\n",
    "Eventually, we will end up in the same situation that Wikidata is in where we will have multiple items with the same label that are disambiguated by their other attributes (classification being the likely chief distinguishing factor). Key examples on the close horizon will be how we deal with minerals such as gold that can be both a chemical element, as defined in Wikidata now, and a mineral commodity in some contexts. Do we have one item called \"gold\" that can be an instance of a number of things or multiple items with the same label that need to be disambiguated?\n",
    "\n",
    "For the immediate future, I'm going to take the approach of constraining the GeoKB to unique labels (including alt labels) and see where we run into problems. The check_item_label() function is something I'm setting up to be run any time we go to create an item so that we ensure uniqueness.\n",
    "\n",
    "The same dynamic applies with properties as well, though those should be more straightforward as we're striving for clear, unambiguous semantics anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SPARQL Queries\n",
    "def query_by_item_label(label: str, include_aliases: bool = True) -> str:\n",
    "    label_criteria = 'rdfs:label|skos:altLabel'\n",
    "    if not include_aliases:\n",
    "        label_criteria = 'rdfs:label'\n",
    "    query_string = \"\"\"\n",
    "        SELECT ?item ?itemLabel ?itemDescription ?itemAltLabel WHERE{  \n",
    "        ?item %s \"%s\"@en.\n",
    "        SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }    \n",
    "        }\n",
    "    \"\"\" % (label_criteria, label)\n",
    "\n",
    "    return query_string\n",
    "\n",
    "def query_item_subclasses(item_id: str, subclass_property_id: str = 'P13') -> str:\n",
    "    query_string = \"\"\"\n",
    "        SELECT ?item ?itemLabel (GROUP_CONCAT(DISTINCT ?subclassOf; SEPARATOR=\",\") as ?subclasses)\n",
    "        {\n",
    "        VALUES (?item) {(wd:%s)}\n",
    "        OPTIONAL {\n",
    "            ?item wdt:%s ?subclassOf\n",
    "        }\n",
    "        SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "        } GROUP BY ?item ?itemLabel\n",
    "    \"\"\" % (item_id, subclass_property_id)\n",
    "\n",
    "    return query_string\n",
    "\n",
    "property_query = \"\"\"\n",
    "SELECT ?property ?propertyLabel ?propertyDescription ?propertyAltLabel WHERE {\n",
    "    ?property a wikibase:Property .\n",
    "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\" .}\n",
    " }\n",
    " \"\"\"\n",
    "\n",
    "\n",
    "def sparql_query(endpoint: str, output: str, query: str):\n",
    "    sparql = SPARQLWrapper(endpoint)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "\n",
    "    sparql.setQuery(query)\n",
    "    results = sparql.queryAndConvert()\n",
    "\n",
    "    if output == 'raw':\n",
    "        return results\n",
    "    elif output == 'dataframe':\n",
    "        names = results[\"head\"][\"vars\"]\n",
    "        data = []\n",
    "        for name in names:\n",
    "            property_value = nested_lookup('value', nested_lookup(name, results['results']['bindings']))\n",
    "            if not property_value:\n",
    "                data.append(None)\n",
    "            else:\n",
    "                data.append(property_value)\n",
    "\n",
    "        return pd.DataFrame.from_dict(dict(zip(names, data)))\n",
    "\n",
    "def get_wb(site_name: str, language='en'):\n",
    "    site = pwb.Site(language, site_name)\n",
    "    site.login()\n",
    "    return site\n",
    "\n",
    "def check_item_label(labels: dict, response: str = 'id'):\n",
    "    label = labels['en']\n",
    "\n",
    "    query_string = query_by_item_label(label=label)\n",
    "\n",
    "    query_results = sparql_query(\n",
    "        endpoint=sparql_endpoint,\n",
    "        output='raw',\n",
    "        query=query_string\n",
    "    )\n",
    "\n",
    "    if not query_results[\"results\"][\"bindings\"]:\n",
    "        return\n",
    "    \n",
    "    if len(query_results[\"results\"][\"bindings\"]) > 1:\n",
    "        raise ValueError(f\"More than one item with the label: {labels}\")\n",
    "    \n",
    "    if response == 'id':\n",
    "        return query_results[\"results\"][\"bindings\"][0][\"item\"][\"value\"].split('/')[-1]\n",
    "\n",
    "    return query_results[\"results\"][\"bindings\"][0]\n",
    "\n",
    "def get_entity(\n",
    "        site: pwb.APISite, \n",
    "        entity_id: str = None, \n",
    "        entity_type: str = 'item', \n",
    "        data_type: str = 'wikibase-item'):\n",
    "\n",
    "    if entity_id and entity_id.startswith('Q'):\n",
    "        entity_type = 'item'\n",
    "    elif entity_id and entity_id.startswith('P'):\n",
    "        entity_type = 'property'\n",
    "    else:\n",
    "        entity_type = entity_type\n",
    "\n",
    "    if entity_type == 'item':\n",
    "        return pwb.ItemPage(\n",
    "            site=site.data_repository(),\n",
    "            title=entity_id\n",
    "        )\n",
    "    elif entity_type == 'property':\n",
    "        return pwb.PropertyPage(\n",
    "            source=site.data_repository(),\n",
    "            title=entity_id,\n",
    "            datatype=data_type\n",
    "        )\n",
    "\n",
    "def edit_labels(\n",
    "        site: pwb.APISite, \n",
    "        labels: dict, \n",
    "        prov_statement: str, \n",
    "        entity_type: str = 'item', \n",
    "        data_type: str = 'wikibase-item'\n",
    "    ) -> str:\n",
    "    try:\n",
    "        entity_id = check_item_label(labels=labels)\n",
    "    except Exception as e:\n",
    "        raise ValueError(\"Problem in running query for item on labels\")\n",
    "\n",
    "    entity = get_entity(\n",
    "        site=site,\n",
    "        entity_id=entity_id,\n",
    "        entity_type=entity_type,\n",
    "        data_type=data_type\n",
    "    )\n",
    "\n",
    "    entity.editLabels(\n",
    "        labels=labels, \n",
    "        summary=prov_statement\n",
    "    )\n",
    "\n",
    "    return entity.getID()\n",
    "\n",
    "def edit_descriptions(site: pwb.APISite, entity_id: str, descriptions: dict, prov_statement: str):\n",
    "    entity = get_entity(\n",
    "        site=site,\n",
    "        entity_id=entity_id\n",
    "    )\n",
    "\n",
    "    if entity.getID() == '-1':\n",
    "        raise ValueError('Entity does not yet exist, create it first')\n",
    "\n",
    "    entity.editDescriptions(\n",
    "        descriptions=descriptions,\n",
    "        summary=prov_statement,\n",
    "    )\n",
    "    return entity.get()\n",
    "\n",
    "def edit_aliases(site: pwb.APISite, entity_id: str, aliases: dict, prov_statement: str):\n",
    "    entity = get_entity(\n",
    "        site=site,\n",
    "        entity_id=entity_id\n",
    "    )\n",
    "\n",
    "    if entity.getID() == '-1':\n",
    "        raise ValueError('Entity does not yet exist, create it first')\n",
    "\n",
    "    entity.editAliases(\n",
    "        aliases=aliases,\n",
    "        summary=prov_statement,\n",
    "    )\n",
    "    return entity.get()\n",
    "\n",
    "def process_item(\n",
    "        site: pwb.APISite, \n",
    "        label: str, \n",
    "        prov_statement: str,\n",
    "        description: str = None, \n",
    "        aliases: list = []\n",
    "    ):\n",
    "\n",
    "    # Assume English language for now\n",
    "    label_dict = {'en': label}\n",
    "\n",
    "    check_item = check_item_label(\n",
    "        labels=label_dict,\n",
    "        response='raw'\n",
    "    )\n",
    "\n",
    "    if not check_item:\n",
    "        entity_id = edit_labels(\n",
    "            site=site,\n",
    "            labels={'en': 'copper'},\n",
    "            prov_statement=prov_statement,\n",
    "            entity_type='item'\n",
    "        )\n",
    "        missing_description = description\n",
    "        missing_aliases = aliases\n",
    "    else:\n",
    "        entity_id = check_item['item']['value'].split('/')[-1]\n",
    "        missing_description = description if description != check_item['itemDescription']['value'] else None\n",
    "        existing_aliases = [i.strip() for i in check_item['itemAltLabel']['value'].split(',')]\n",
    "        missing_aliases = list(set(aliases) - set(existing_aliases))\n",
    "\n",
    "    if missing_description:\n",
    "        edit_descriptions(\n",
    "            site=site,\n",
    "            entity_id=entity_id,\n",
    "            descriptions={'en': missing_description},\n",
    "            prov_statement=f'Adding description for {label}'\n",
    "        )\n",
    "    \n",
    "    if missing_aliases:\n",
    "        edit_aliases(\n",
    "            site=site,\n",
    "            entity_id=entity_id,\n",
    "            aliases={'en': ['Cu','Copper']},\n",
    "            prov_statement=f'Adding aliases for {label}'\n",
    "        )\n",
    "\n",
    "# Still problematic here with ItemPage.get() after adding claims\n",
    "def add_claim(site: pwb.APISite, subject_item_id: str, property_id: str, claim_value: str, prov_statement: str):\n",
    "    repo = site.data_repository()\n",
    "\n",
    "    # Establish connection to item\n",
    "    subject_item = pwb.ItemPage(repo, subject_item_id)\n",
    "    if not subject_item.exists():\n",
    "        raise ValueError(f'Item does not exist: {subject_item_id}')\n",
    "    \n",
    "    # Establish connection to property\n",
    "    property_item = pwb.PropertyPage(repo, property_id)\n",
    "    try:\n",
    "        property_datatype = property_item.get()['datatype']\n",
    "        subject_claim = pwb.Claim(repo, property_id)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f'Property does not exist: {property_id}')\n",
    "    \n",
    "    if property_datatype == 'wikibase-item':\n",
    "        # Get item target and verify exists\n",
    "        claim_object = pwb.ItemPage(repo, claim_value)\n",
    "        if not claim_object.exists():\n",
    "            raise ValueError(f\"Object item does not exist: {claim_value}\")\n",
    "    else:\n",
    "        # Need to handle other cases where we property test/format an appropriate claim_target response\n",
    "        return\n",
    "\n",
    "    # Set the target for the claim\n",
    "    subject_claim.setTarget(claim_object)\n",
    "    # Need to handle additional work of adding references and qualifiers\n",
    "\n",
    "    # Commit the claim to wikibase\n",
    "    try:\n",
    "        subject_item.addClaim(subject_claim, summary=prov_statement)\n",
    "    except Exception as e:\n",
    "        raise ValueError(\"Claim already exists\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foundational Properties and Classes\n",
    "\n",
    "Looking toward the initial use cases for the GeoKB, we need to lay down a basic structure for properties and classification such that the items we need to introduce will all have an appropriate instance of claim pointing to a reasonable concept for basic organization. We could go on forever trying to get the semantics just right and aligned with as many sources of definition as possible, but we'll ease into that level of sophistication over time. In the near term, I've started a Google Sheet to contain the basic structure to get our knowledgebase initialized. We will likely need to iterate on this several times to get it right, and we will doubtless miss some things.\n",
    "\n",
    "There are lots of ways to spin up these details, but a Google Sheet seems simple enough, it can be edited by multiple people, and we can read it as a CSV and process it here in the notebook. I've built this with a Properties and a Classification sheet that we'll iterate on as we improve the foundation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ref_table(reference, sheet_id=geokb_init_sheet_id):\n",
    "    return f'https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={reference}'\n",
    "\n",
    "geokb_properties = pd.read_csv(ref_table('Properties'))\n",
    "geokb_classes = pd.read_csv(ref_table('Classification'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(geokb_properties)\n",
    "display(geokb_classes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to Wikibase\n",
    "\n",
    "Many read operations can be accommodated by the public SPARQL interface with no need for specialized wikibase connections. At the moment, the Elasticsearch part of the tech stack is not connecting properly, and some of the dependent API functionality in the pywikibot package is not working. The write operations for properties and items are working fine, and we need to establish a connection to the APISite via pywikibot and the user/password bot config we set up previously.\n",
    "\n",
    "These create user-config.py and user-password.py files in the directory from which pywikibot is run, which is not the most secure way to handle things. We may experiment with the more secure OAuth methods down the road once we get the mechanics worked out. The following sets up the connection for later use and tests it for functionality. I'm not sure yet on the significance of the errors I'm seeing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required connection points in the pwb API\n",
    "geokb_site = get_wb('geokb')\n",
    "\n",
    "print(type(geokb_site))\n",
    "print(type(geokb_site.data_repository()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Knowledgebase\n",
    "\n",
    "We may or may not be starting from a blank slate Wikibase installation. We know we need to have everything laid out in our Properties and Classification tables, but some of it may already exist. We also need to build claims on these items that use property and item identifiers, so we need to determine exactly what's in the current system. My initial attempt to use the search functionality based on pywikibot and the currently non-functional API failed, so I'm experimenting with a SPARQL approach. This is somewhat handy in that we can leverage the SPARQL query builder to figure out our queries, first with a test against the full Wikidata platform and then our own instance. Those queries can be pulled in here based on how I set up the function to get us a result to work from.\n",
    "\n",
    "## Building properties and items\n",
    "\n",
    "I left off here after proving that I can at least create items. This last bit was following the [tutorial](https://www.wikidata.org/wiki/Wikidata:Pywikibot_-_Python_3_Tutorial/Labels) on building and editing items to include more rich provenance (history). I'll come back to build this into functional logic.\n",
    "\n",
    "### Deletion\n",
    "\n",
    "After some reading on this, it looks like deleting pages in a Mediawiki instance can only be done by users with administrative privileges. This is probably a good safeguard for us to leverage, but we need to suss out a usable workflow. There appears to be some type of functionality to mark a page for deletion, presumably with administrator action to concur on the action and execute it.\n",
    "\n",
    "### Property creation and maintenance\n",
    "\n",
    "I can get a list of all properties in the instance, but I can't yet figure out a way to get a list of all items. With a full set of properties, we can figure out what we have that's in baseline set, get rid of anything extraneous, and add anything new. We could do the same with items to a certain point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_properties = sparql_query(sparql_endpoint, 'dataframe', property_query)\n",
    "df_properties['id'] = df_properties.property.apply(lambda x: x.split('/')[-1])\n",
    "df_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing properties\n",
    "missing_properties = geokb_properties[~geokb_properties.label.isin(df_properties.propertyLabel)]\n",
    "missing_properties"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item creation and development\n",
    "\n",
    "I've reworked a process_item() function using other foundational functions that apply for both items and properties (entities). Our use case for creating items of all types, whether part of the foundation or otherwise, will often involve simply sending in a label along with optional (but strongly encouraged), description, and aliases. We will also re-run this over time when we have new descriptions and aliases for a given set of things to update these basic elements. For now, we assume all English language values, but we will need to deal with multiple languages relatively soon (e.g., mine names in other languages are common).\n",
    "\n",
    "The following test shows the basic concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_item(\n",
    "    site=geokb_site,\n",
    "    label='copper',\n",
    "    prov_statement='Adding item for copper as mineral element and commodity',\n",
    "    description='chemical element with symbol Cu and atomic number 29',\n",
    "    aliases=['Cu','Copper']\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Claims\n",
    "\n",
    "I'm stuck here (see below) trying to work out the process from https://www.wikidata.org/wiki/Wikidata:Pywikibot_-_Python_3_Tutorial/Setting_statements and looking at what J. Olivero did with his initial bot code. It should work, but I'm making the items nonfunctional somehow.\n",
    "\n",
    "I'm leaving off here with a problem in this function. An error is raised with trying to get an item where I've set a claim with the function previously. It is erroring out with an AttributeError on the first step of running a get() on the item. addClaim() appears to work as the claim shows up on the item in the GUI as expecte, but I can then no longer run a get() on the item without this error in code. I also get the same error if I add the claim using the GUI. As soon as I get rid of the claim, I can get() the item just fine. And even through item.get() fails after adding a claim, I can still run a SPARQL query to retrieve the item and a subclass of claim. But since I can't run an item.get() any more, after a claim is on board, that means I can't add any more claims using this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_claim(\n",
    "    site=geokb_site,\n",
    "    subject_item_id='Q33',\n",
    "    property_id='P2',\n",
    "    claim_value='Q3',\n",
    "    prov_statement='Classifying item as subclass of entity'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "DataSite instance has no attribute 'entity_sources'\n",
      "DataSite instance has no attribute 'entity_sources'\n"
     ]
    }
   ],
   "source": [
    "import pywikibot as pwb\n",
    "\n",
    "test_site = pwb.Site('en', 'geokb')\n",
    "test_site.login()\n",
    "test_repo = test_site.data_repository()\n",
    "\n",
    "test_item_q5 = pwb.ItemPage(test_repo, 'Q5') # No claims yet - https://wiki.demo5280.com/wiki/Item:Q5\n",
    "try:\n",
    "    print(test_item_q5.exists())\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "test_item_q6 = pwb.ItemPage(test_repo, 'Q6') # Subclass claim on item - https://wiki.demo5280.com/wiki/Item:Q6\n",
    "try:\n",
    "    print(test_item_q6.exists())\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "test_property = pwb.PropertyPage(test_repo, 'P2') # Property with a claim - https://wiki.demo5280.com/wiki/Property:P2\n",
    "try:\n",
    "    print(test_property.get()['datatype'])\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ItemPage' object has no attribute 'latest_revision_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m instance_of_claim\u001b[39m.\u001b[39msetTarget(mineral)\n\u001b[1;32m     11\u001b[0m \u001b[39m# subject_claim.setTarget(mineral)\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m gold\u001b[39m.\u001b[39;49maddClaim(instance_of_claim, summary\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mTesting adding instance of claim\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     14\u001b[0m \u001b[39m# gold.addClaim(subject_claim, summary='Testing adding subject matter claim')\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/geokb/lib/python3.11/site-packages/pywikibot/page/_decorators.py:55\u001b[0m, in \u001b[0;36m_allow_asynchronous.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m     pywikibot\u001b[39m.\u001b[39masync_request(handle, func, \u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     54\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 55\u001b[0m     handle(func, \u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/geokb/lib/python3.11/site-packages/pywikibot/page/_decorators.py:34\u001b[0m, in \u001b[0;36m_allow_asynchronous.<locals>.handle\u001b[0;34m(func, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m err \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 34\u001b[0m     func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     35\u001b[0m \u001b[39m# TODO: other \"expected\" error types to catch?\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[39mexcept\u001b[39;00m Error \u001b[39mas\u001b[39;00m edit_err:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/geokb/lib/python3.11/site-packages/pywikibot/page/_wikibase.py:748\u001b[0m, in \u001b[0;36mWikibasePage.addClaim\u001b[0;34m(self, claim, bot, **kwargs)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[39mif\u001b[39;00m claim\u001b[39m.\u001b[39mon_item \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    746\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    747\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mThe provided Claim instance is already used in an entity\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 748\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrepo\u001b[39m.\u001b[39;49maddClaim(\u001b[39mself\u001b[39;49m, claim, bot\u001b[39m=\u001b[39;49mbot, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    749\u001b[0m claim\u001b[39m.\u001b[39mon_item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/geokb/lib/python3.11/site-packages/pywikibot/site/_decorators.py:92\u001b[0m, in \u001b[0;36mneed_right.<locals>.decorator.<locals>.callee\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39melif\u001b[39;00m right \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_right(right):\n\u001b[1;32m     89\u001b[0m     \u001b[39mraise\u001b[39;00m UserRightsError(\u001b[39m'\u001b[39m\u001b[39mUser \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m does not have required \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     90\u001b[0m                           \u001b[39m'\u001b[39m\u001b[39muser right \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     91\u001b[0m                           \u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muser(), right))\n\u001b[0;32m---> 92\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/geokb/lib/python3.11/site-packages/pywikibot/site/_datasite.py:341\u001b[0m, in \u001b[0;36mDataSite.addClaim\u001b[0;34m(self, entity, claim, bot, summary)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[39mAdd a claim.\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[39m:param summary: Edit summary\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    338\u001b[0m claim\u001b[39m.\u001b[39msnak \u001b[39m=\u001b[39m entity\u001b[39m.\u001b[39mgetID() \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m$\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(uuid\u001b[39m.\u001b[39muuid4())\n\u001b[1;32m    339\u001b[0m params \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39maction\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mwbsetclaim\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    340\u001b[0m           \u001b[39m'\u001b[39m\u001b[39mclaim\u001b[39m\u001b[39m'\u001b[39m: json\u001b[39m.\u001b[39mdumps(claim\u001b[39m.\u001b[39mtoJSON()),\n\u001b[0;32m--> 341\u001b[0m           \u001b[39m'\u001b[39m\u001b[39mbaserevid\u001b[39m\u001b[39m'\u001b[39m: entity\u001b[39m.\u001b[39;49mlatest_revision_id,\n\u001b[1;32m    342\u001b[0m           \u001b[39m'\u001b[39m\u001b[39msummary\u001b[39m\u001b[39m'\u001b[39m: summary,\n\u001b[1;32m    343\u001b[0m           \u001b[39m'\u001b[39m\u001b[39mtoken\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokens[\u001b[39m'\u001b[39m\u001b[39mcsrf\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m    344\u001b[0m           \u001b[39m'\u001b[39m\u001b[39mbot\u001b[39m\u001b[39m'\u001b[39m: bot,\n\u001b[1;32m    345\u001b[0m           }\n\u001b[1;32m    346\u001b[0m req \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msimple_request(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[1;32m    347\u001b[0m data \u001b[39m=\u001b[39m req\u001b[39m.\u001b[39msubmit()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/geokb/lib/python3.11/site-packages/pywikibot/page/_wikibase.py:137\u001b[0m, in \u001b[0;36mWikibaseEntity.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, name)\n\u001b[1;32m    135\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget()[name]\n\u001b[0;32m--> 137\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    138\u001b[0m                      \u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ItemPage' object has no attribute 'latest_revision_id'"
     ]
    }
   ],
   "source": [
    "gold = pwb.ItemPage(test_repo, 'Q34')\n",
    "material = pwb.ItemPage(test_repo, 'Q19')\n",
    "mineral = pwb.ItemPage(test_repo, 'Q25')\n",
    "instance_of = pwb.PropertyPage(test_repo, 'P2')\n",
    "subject_matter = pwb.PropertyPage(test_repo, 'P23')\n",
    "\n",
    "instance_of_claim = pwb.Claim(test_repo, 'P2')\n",
    "# subject_claim = pwb.Claim(test_repo, 'P23')\n",
    "\n",
    "instance_of_claim.setTarget(mineral)\n",
    "# subject_claim.setTarget(mineral)\n",
    "\n",
    "gold.addClaim(subject_claim, summary='Testing adding instance of claim')\n",
    "# gold.addClaim(subject_claim, summary='Testing adding subject matter claim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold.latest_revision_id"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 ('geokb')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bf3b427550abc636388c726c622fd95adcdc42f0880499fad9fd5e67fc347911"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
