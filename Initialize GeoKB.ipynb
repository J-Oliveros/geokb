{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a work in progress process for initializing the Geoscience Knowledgebase with a set of properties and foundational semantics that establishes a base to build from in curating geoscientific knowledge. Our initial use cases have to do with integrating mineral occurrence information along with document references contributing to those and other facts and concepts associated with conducting mineral resource assessment. In order to build a useful knowledge graph on these concepts, though, we also need to tie in lots of other things needed for the claims associated with these things to legitimately link to other things.\n",
    "\n",
    "For instance, we use NI \"43-101 Technical Reports\" and a newer \"SK-1300 Technical Report\" as sources for claims associated with mining projects/properties such as geographic location, mineral commodities identified and/or extracted, figures indicating estimates of ore grade and tonnage, and other details. These are technical geoscientific reports required by the Canadian and U.S. governments, respectively. We need an \"instance of\" (rdf:type) claim on everything like this in the system. While we could simply create items in the graph to represent these two classes with no further classification, it is useful to \"work back up the semantic hierarchy\" for as many concepts as we can as far as we need to in order for the information we are recording in the GeoKB to be understandable in the broader global knowledge commons (Wikidata and/or other efforts).\n",
    "\n",
    "The initialization process here is designed to give us a semantic baseline to work from as we pull in the information and connections we really care about within this context. We're taking a pragmatic approach that is slightly more rigorous (and certainly more streamlined) than the wild west of Wikidata but somewhere short of endlessly academic. We have to get a whole bunch of information into the GeoKB to support analytical use, so we make a best effort to align what we have with mature ontologies and namespaces, knowing we'll have to evolve it over time. The notebook approach on this gives us a good basis to record our reasoning and the places we have to make pragmatic tradeoffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pywikibot as pwb\n",
    "import json\n",
    "import pandas as pd\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from nested_lookup import nested_lookup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "If we want to run this and other notebooks as Lambdas eventually, we'll need to set up some paramters. I moved the stuff we want to keep somewhat secret to environment variables as a safety measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "sparql_endpoint = os.environ['SPARQL_ENDPOINT']\n",
    "wb_domain = os.environ['WB_DOMAIN']\n",
    "geokb_init_sheet_id = '1dbuKc4cZJz0YY81B2xWXM5fId6gWgzmQar3hg3CI0Rw'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "All or most of these functions should be movable to the abstract Wikibase management python package we are designing. That needs to be applicable to the GeoKB but generic enough to apply in other types of domains and circumstances. There are other communities doing similar work such as the wikidataintegrator project in the health sciences. We just found a need to start from the basics of pywikibot and how it operates.\n",
    "\n",
    "I realize I'm introducing what could be higher overhead than necessary here with use of Pandas and other specialized packages. We can reevaluate that at the foundational level of the KB management package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wb(site_name: str, language='en'):\n",
    "    site = pwb.Site(language, site_name)\n",
    "    site.login()\n",
    "    return site\n",
    "\n",
    "def build_entity(\n",
    "        label: str, \n",
    "        description: str, \n",
    "        language: str = 'en',\n",
    "        datatype = None\n",
    "    ) -> dict:\n",
    "    item = {\n",
    "        'labels': {\n",
    "            language: {\n",
    "                'language': language,\n",
    "                'value': label\n",
    "            }\n",
    "        },\n",
    "        'descriptions': {\n",
    "            language: {\n",
    "                'language': language,\n",
    "                'value': description\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if datatype:\n",
    "        # Need to work out our own logic on what datatypes we will accept\n",
    "        item[\"datatype\"] = datatype\n",
    " \n",
    "    return item\n",
    "\n",
    "# I probably need to break this all up so that each element of an item or property gets built individually\n",
    "# Some bot examples establish a connection to a \"Page\" and then add labels, descriptions, and aliases separately\n",
    "# This would allow us to use those same functions in many circumstances like incrementally verifying the baseline\n",
    "def add_entity(entitytype: str, summary: str, site: pwb.APISite, item: dict) -> dict:\n",
    "    if entitytype not in ['item','property']:\n",
    "        raise ValueError\n",
    "    \n",
    "    if entitytype == 'property' and 'datatype' not in item:\n",
    "        raise ValueError\n",
    "\n",
    "    params = {\n",
    "        'action': 'wbeditentity',\n",
    "        'new': entitytype,\n",
    "        'data': json.dumps(item),\n",
    "        'summary': summary,\n",
    "        'token': site.tokens['csrf'],\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        req = site.simple_request(**params)\n",
    "        results = req.submit()\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        item[\"error\"] = e\n",
    "        return e\n",
    "\n",
    "def update_aliases(site: pwb.APISite, entity_id: str, aliases):\n",
    "    item = pwb.ItemPage(site.data_repository(), entity_id)\n",
    "    alias = {\"en\": aliases.split(',') if isinstance(aliases, str) else aliases}\n",
    "\n",
    "    messages = []\n",
    "    for key in alias:\n",
    "        message = \"Settings aliases: {} = '{}'\".format(key, alias[key])\n",
    "        try:\n",
    "            item.editAliases(\n",
    "                {key: alias[key]},\n",
    "                summary=message\n",
    "            )\n",
    "            messages.append({\"message\": message})\n",
    "        except Exception as e:\n",
    "            messages.append({\n",
    "                \"message\": message,\n",
    "                \"error\": e\n",
    "            })\n",
    "    return messages\n",
    "    \n",
    "def sparql_query(endpoint: str, output: str, query: str):\n",
    "    sparql = SPARQLWrapper(endpoint)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "\n",
    "    sparql.setQuery(query)\n",
    "    results = sparql.queryAndConvert()\n",
    "\n",
    "    if output == 'raw':\n",
    "        return results\n",
    "    elif output == 'dataframe':\n",
    "        names = results[\"head\"][\"vars\"]\n",
    "        data = []\n",
    "        for name in names:\n",
    "            property_value = nested_lookup('value', nested_lookup(name, results['results']['bindings']))\n",
    "            if not property_value:\n",
    "                data.append(None)\n",
    "            else:\n",
    "                data.append(property_value)\n",
    "\n",
    "        return pd.DataFrame.from_dict(dict(zip(names, data)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foundational Properties and Classes\n",
    "\n",
    "Looking toward the initial use cases for the GeoKB, we need to lay down a basic structure for properties and classification such that the items we need to introduce will all have an appropriate instance of claim pointing to a reasonable concept for basic organization. We could go on forever trying to get the semantics just right and aligned with as many sources of definition as possible, but we'll ease into that level of sophistication over time. In the near term, I've started a Google Sheet to contain the basic structure to get our knowledgebase initialized. We will likely need to iterate on this several times to get it right, and we will doubtless miss some things.\n",
    "\n",
    "There are lots of ways to spin up these details, but a Google Sheet seems simple enough, it can be edited by multiple people, and we can read it as a CSV and process it here in the notebook. I've built this with a Properties and a Classification sheet that we'll iterate on as we improve the foundation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ref_table(reference, sheet_id=geokb_init_sheet_id):\n",
    "    return f'https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={reference}'\n",
    "\n",
    "geokb_properties = pd.read_csv(ref_table('Properties'))\n",
    "geokb_classes = pd.read_csv(ref_table('Classification'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(geokb_properties)\n",
    "display(geokb_classes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to Wikibase\n",
    "\n",
    "Many read operations can be accommodated by the public SPARQL interface with no need for specialized wikibase connections. At the moment, the Elasticsearch part of the tech stack is not connecting properly, and some of the dependent API functionality in the pywikibot package is not working. The write operations for properties and items are working fine, and we need to establish a connection to the APISite via pywikibot and the user/password bot config we set up previously.\n",
    "\n",
    "These create user-config.py and user-password.py files in the directory from which pywikibot is run, which is not the most secure way to handle things. We may experiment with the more secure OAuth methods down the road once we get the mechanics worked out. The following sets up the connection for later use and tests it for functionality. I'm not sure yet on the significance of the errors I'm seeing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pywikibot.site._apisite.APISite'>\n",
      "<class 'pywikibot.site._datasite.DataSite'>\n"
     ]
    }
   ],
   "source": [
    "# Required connection points in the pwb API\n",
    "geokb_site = get_wb('geokb')\n",
    "\n",
    "print(type(geokb_site))\n",
    "print(type(geokb_site.data_repository()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Knowledgebase\n",
    "\n",
    "We may or may not be starting from a blank slate Wikibase installation. We know we need to have everything laid out in our Properties and Classification tables, but some of it may already exist. We also need to build claims on these items that use property and item identifiers, so we need to determine exactly what's in the current system. My initial attempt to use the search functionality based on pywikibot and the currently non-functional API failed, so I'm experimenting with a SPARQL approach. This is somewhat handy in that we can leverage the SPARQL query builder to figure out our queries, first with a test against the full Wikidata platform and then our own instance. Those queries can be pulled in here based on how I set up the function to get us a result to work from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_by_item_label(label: str) -> str:\n",
    "    query_string = \"\"\"\n",
    "        SELECT ?item ?itemLabel ?itemDescription ?itemAltLabel WHERE{  \n",
    "        ?item ?label \"%s\"@en.  \n",
    "        SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }    \n",
    "        }\n",
    "    \"\"\" % (label)\n",
    "\n",
    "    return query_string\n",
    "\n",
    "# Build on this idea for assembling multiple claims about the same property in tabular form\n",
    "def query_item_subclasses(item_id: str, subclass_property_id: str = 'P13') -> str:\n",
    "    query_string = \"\"\"\n",
    "        SELECT ?item ?itemLabel (GROUP_CONCAT(DISTINCT ?subclassOf; SEPARATOR=\",\") as ?subclasses)\n",
    "        {\n",
    "        VALUES (?item) {(wd:%s)}\n",
    "        OPTIONAL {\n",
    "            ?item wdt:%s ?subclassOf\n",
    "        }\n",
    "        SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "        } GROUP BY ?item ?itemLabel\n",
    "    \"\"\" % (item_id, subclass_property_id)\n",
    "\n",
    "    return query_string\n",
    "\n",
    "\n",
    "property_query = \"\"\"\n",
    "SELECT ?property ?propertyLabel ?propertyDescription ?propertyAltLabel WHERE {\n",
    "    ?property a wikibase:Property .\n",
    "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\" .}\n",
    " }\n",
    " \"\"\"\n",
    "\n",
    "property_query = \"\"\"\n",
    " SELECT ?item ?itemLabel ?prop ?propertyLabel ?propertyValue ?propertyValueLabel\n",
    "{ \n",
    "  VALUES (?item) {(wd:Q3)}\n",
    "  OPTIONAL {\n",
    "    ?item ?prop ?statement . \n",
    "    ?statement ?ps ?propertyValue . \n",
    "    ?property wikibase:claim ?prop . \n",
    "    ?property wikibase:statementProperty ?ps . \n",
    "  } \n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\" } \n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparql_query(\n",
    "    endpoint=sparql_endpoint,\n",
    "    output='dataframe',\n",
    "    query=query_item_subclasses('Q3', 'P13')\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can get a list of all properties in the instance, but I can't yet figure out a way to get a list of all items. With a full set of properties, we can figure out what we have that's in baseline set, get rid of anything extraneous, and add anything new. We could do the same with items to a certain point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_properties = sparql_query(sparql_endpoint, 'dataframe', property_query)\n",
    "df_properties['id'] = df_properties.property.apply(lambda x: x.split('/')[-1])\n",
    "df_properties"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, the best I've come up with is a query on a given item by label or other criteria. I need to work out the best way to check through our baseline, and iterating through the whole list seems kind of dumb. For whatever reason, I can't apply a similar criteria for items that I did for properties like...\n",
    "\n",
    "e.g., `?item a wikibase:Item`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sparql_query(sparql_endpoint, 'dataframe', query_by_item_label('human'))\n",
    "df['item'] = df.item.apply(lambda x: x.split('/')[-1])\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building properties and items\n",
    "\n",
    "I left off here after proving that I can at least create items. This last bit was following the [tutorial](https://www.wikidata.org/wiki/Wikidata:Pywikibot_-_Python_3_Tutorial/Labels) on building and editing items to include more rich provenance (history). I'll come back to build this into functional logic.\n",
    "\n",
    "### Deletion\n",
    "\n",
    "After some reading on this, it looks like deleting pages in a Mediawiki instance can only be done by users with administrative privileges. This is probably a good safeguard for us to leverage, but we need to suss out a usable workflow. There appears to be some type of functionality to mark a page for deletion, presumably with administrator action to concur on the action and execute it.\n",
    "\n",
    "### Looping vs. something more efficient\n",
    "\n",
    "The pywikibot engine appears to impose some throttling on looped operations like adding entities. This doesn't matter when running something like a basic initialization process with a handful of properties and classification items. It might or might not matter when we get to loading thousands of reference items and the many other things we need to create. The building of structured and semantically explicit knowledge is somewhat inherently slow and deliberate anyway, so we may not really care if a process has to run all night to load new information that won't change much after that point except incrementally. But we do need to better understand what's imposed by the infrastructure and how we best handle our use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing properties\n",
    "missing_properties = geokb_properties[~geokb_properties.label.isin(df_properties.propertyLabel)]\n",
    "missing_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in missing_properties.iterrows():\n",
    "    wb_entity = build_entity(\n",
    "        label=row.label,\n",
    "        description=row.description,\n",
    "        datatype=row.datatype\n",
    "    )\n",
    "\n",
    "    results = add_entity(\n",
    "        entitytype='property',\n",
    "        summary='Adding new property from GeoKB initialization spreadsheet',\n",
    "        site=geokb_site,\n",
    "        item=wb_entity\n",
    "    )\n",
    "\n",
    "    display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to come back to better understand how to deal with property changes and deprecation over time.\n",
    "\n",
    "bad_prop = pwb.PropertyPage(geokb_site.data_repository(), 'P1')\n",
    "bad_prop.get()\n",
    "# bad_prop.delete()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item creation and development\n",
    "\n",
    "Since the looping process to build items is somewhat slow and deliberative with pywikibot anyway, it doesn't matter if we need to check each classification item from our initialization spreadsheet, decide if it's already in the GeoKB, and then build if if necessary.\n",
    "\n",
    "Right now, this seems like a slow and clunky way to introduce things to the GeoKB. I ended up breaking out the piece on adding aliases because I kept failing to get the item encoding right for the `pwb.simple_request()` method. But this is also an area where recording aliases individually for items as they come up is going to be something we do pretty often, and having these inserted with a summary statement for provenance will be useful."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Come Back Later\n",
    "\n",
    "Everything from here through the claims section below is not the way I want to run this. Rather, I think I need to simplify by running through each item in the baseline set of classification entities to do the following:\n",
    "1. Check that the item exists...\n",
    "2. if not exists, create it with a new function that instantiates the page and then incrementally adds label, description, and aliases\n",
    "3. If exists, verify that description and aliases align with the init spreadsheet and reset if needed\n",
    "4. If exists, query for the subclasses on the ID and set/reset if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is wasted processing to first check everything and then go back and do stuff; \n",
    "# need to just run through everything and do it\n",
    "geokb_items = []\n",
    "geokb_subclass_properties = []\n",
    "for index, row in geokb_classes.iterrows():\n",
    "    df_labeled_item = sparql_query(\n",
    "        endpoint=sparql_endpoint,\n",
    "        output='dataframe',\n",
    "        query=query_by_item_label(label=row.label)\n",
    "    )\n",
    "    if not df_labeled_item.empty:\n",
    "        geokb_items.append(df_labeled_item)\n",
    "        item_id = df_labeled_item.iloc[0]['item'].split('/')[-1]\n",
    "        geokb_subclass_properties.append(\n",
    "            sparql_query(\n",
    "                endpoint=sparql_endpoint,\n",
    "                output='dataframe',\n",
    "                query=query_item_subclasses(\n",
    "                    item_id=item_id,\n",
    "                    # Come back to this\n",
    "                    subclass_property_id='P13'\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "df_geokb_items = pd.concat(geokb_items)\n",
    "df_geokb_items = df_geokb_items.drop_duplicates()\n",
    "df_geokb_items['item'] = df_geokb_items.item.apply(lambda x: x.split('/')[-1])\n",
    "\n",
    "df_check_baseline_classes = pd.merge(\n",
    "    left=geokb_classes,\n",
    "    right=df_geokb_items,\n",
    "    how='left',\n",
    "    left_on='label',\n",
    "    right_on='itemLabel',\n",
    "    indicator=True\n",
    ")\n",
    "\n",
    "df_check_baseline_classes['aliases'] = df_check_baseline_classes.aliases.apply(lambda x: [i.strip().lower() for i in x.split(',')] if isinstance(x, str) else [])\n",
    "df_check_baseline_classes['itemAltLabel'] = df_check_baseline_classes.itemAltLabel.apply(lambda x: [i.strip().lower() for i in x.split(',')] if isinstance(x, str) else [])\n",
    "\n",
    "df_check_baseline_classes['missing_aliases'] = df_check_baseline_classes.apply(\n",
    "    lambda x: list(set(x.aliases) - set(x.itemAltLabel)),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "missing_classes = df_check_baseline_classes[df_check_baseline_classes._merge != 'both']\n",
    "missing_aliases = df_check_baseline_classes[df_check_baseline_classes.missing_aliases.str.len() > 0]\n",
    "modified_descriptions = df_check_baseline_classes[\n",
    "    df_check_baseline_classes.itemDescription != df_check_baseline_classes.description\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geokb_item_subclasses = pd.concat(geokb_subclass_properties)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geokb_item_subclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This was getting way too complicated, redo\n",
    "\n",
    "geokb_ids = []\n",
    "\n",
    "for index, row in geokb_classes.iterrows():\n",
    "    query_results = sparql_query(\n",
    "        endpoint=sparql_endpoint,\n",
    "        output='raw',\n",
    "        query=query_by_item_label(label=row.label)\n",
    "    )\n",
    "\n",
    "    item_id = None\n",
    "    item_aliases = None\n",
    "\n",
    "    if len(query_results['results']['bindings']) > 1:\n",
    "        print(\"MORE THAN ONE RESULT:\", row.label)\n",
    "    elif len(query_results['results']['bindings']) == 1:\n",
    "        item = query_results['results']['bindings'][0]\n",
    "        item_id = item['item']['value'].split('/')[-1]\n",
    "        if 'itemAltLabel' in item:\n",
    "            item_aliases = item['itemAltLabel']['value']\n",
    "        print(\"ITEM EXISTS\", row.label, item_id)\n",
    "    elif len(query_results['results']['bindings']) == 0:\n",
    "        response = add_entity(\n",
    "            entitytype='item',\n",
    "            summary='Adding classification item from GeoKB initialization spreadsheet',\n",
    "            site=geokb_site,\n",
    "            item=build_entity(\n",
    "                label=row.label,\n",
    "                description=row.description\n",
    "            )\n",
    "        )\n",
    "        item_id = response['entity']['id']\n",
    "        print(\"ADDED ITEM\", row.label, item_id)\n",
    "\n",
    "    geokb_ids.append({\n",
    "        'label': row.label,\n",
    "        'geokb_id': item_id\n",
    "    })\n",
    "\n",
    "    if isinstance(row.aliases, str):\n",
    "        aliases_to_update = row.aliases.split(',')\n",
    "        if item_aliases:\n",
    "            aliases_to_update = list(set(item_aliases) - set(aliases_to_update))\n",
    "        if aliases_to_update:\n",
    "            display(update_aliases(\n",
    "                site=geokb_site,\n",
    "                entity_id=item_id,\n",
    "                aliases=aliases_to_update\n",
    "            ))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Claims\n",
    "\n",
    "I'm stuck here (see below) trying to work out the process from https://www.wikidata.org/wiki/Wikidata:Pywikibot_-_Python_3_Tutorial/Setting_statements and looking at what J. Olivero did with his initial bot code. It should work, but I'm making the items nonfunctional somehow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_claim(site: pwb.APISite, subject_item_id: str, property_id: str, claim_value: str, prov_statement: str):\n",
    "    repo = site.data_repository()\n",
    "\n",
    "    # Establish connection to item\n",
    "    subject_item = pwb.ItemPage(repo, subject_item_id)\n",
    "    return subject_item\n",
    "    # Verify item exists\n",
    "    try:\n",
    "        subject_item.get()\n",
    "    except Exception as e:\n",
    "        raise ValueError(e)\n",
    "    \n",
    "    # Establish connection to property\n",
    "    property_item = pwb.PropertyPage(repo, property_id)\n",
    "    # Verify property exists and get datatype\n",
    "    try:\n",
    "        property_datatype = property_item.get()['datatype']\n",
    "        subject_claim = pwb.Claim(repo, property_id)\n",
    "    except Exception as e:\n",
    "        raise ValueError(e)\n",
    "    \n",
    "    if property_datatype == 'wikibase-item':\n",
    "        # Get item target and verify exists\n",
    "        claim_object = pwb.ItemPage(repo, claim_value)\n",
    "        try:\n",
    "            claim_object.get()\n",
    "        except Exception as e:\n",
    "            raise ValueError(e)\n",
    "    else:\n",
    "        # Need to handle other cases where we property test/format an appropriate claim_target response\n",
    "        return\n",
    "\n",
    "    # Set the target for the claim\n",
    "    subject_claim.setTarget(claim_object)\n",
    "    # Need to handle additional work of adding references and qualifiers\n",
    "\n",
    "    # Commit the claim to wikibase\n",
    "    try:\n",
    "        subject_item.addClaim(subject_claim, summary=prov_statement)\n",
    "    except Exception as e:\n",
    "        raise ValueError(\"Claim already exists\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm leaving off here with a problem in this function. An error is raised with trying to get an item where I've set a claim with the function previously. It is erroring out with an AttributeError on the first step of running a get() on the item. addClaim() appears to work as the claim shows up on the item in the GUI as expecte, but I can then no longer run a get() on the item without this error in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_claim(\n",
    "    site=geokb_site,\n",
    "    subject_item_id='Q7', # organization\n",
    "    property_id='P13', # subclass of\n",
    "    claim_value='Q5', # entity\n",
    "    prov_statement='Classifying item as subclass of entity'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_item = pwb.ItemPage(geokb_site.data_repository(), 'Q33')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataSite instance has no attribute 'entity_sources'\n"
     ]
    }
   ],
   "source": [
    "# I have no idea what this error means\n",
    "try:\n",
    "    test_item.get()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 ('geokb')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bf3b427550abc636388c726c622fd95adcdc42f0880499fad9fd5e67fc347911"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
