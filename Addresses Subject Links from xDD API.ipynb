{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is another experiment in the relatively crude process of building better metadata for items representing documents/publications in the GeoKB from the processing pipeline they are run through in xDD. I still need to run back through and clean up some previous examples as we refine how we want to go about this process in production.\n",
    "\n",
    "We now have representations for all of the NI 43-101 Technical Reports and all USGS Series Reports (plus the majority of USGS-authored journal articles) in the GeoKB. All of those are piped through xDD (GeoDeepDive) where several pipelines are run, including some basic work to identify key scientific concepts that are addressed (or at least mentioned) in the documents. For faily unique terms such as lithologies/rock types and minerals, this can give us a fairly good indication of where documents address important subject matter. They are also terms that are not currently found in metadata from the Pubs Warehouse or our Zotero-managed technical reports. Pulling them in dynamically and adding them to the records for these pubs may prove a useful research aid.\n",
    "\n",
    "The basic philosphpy for this approach is to have the knowledge graph essentially drive itself. We start from things the knowledge graph knows about - documents and geoscientific concepts, consult a third party processing infrastructure that also knows about those things, and record the linkages suggested back within the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from wbmaker import WikibaseConnection\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "geokb = WikibaseConnection('GEOKB_CLOUD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexed Documents\n",
    "In this exercise, we are extracting key subjects that have been identified in documents indexed through the xDD pipelines and linking those to the representations for those documents in the GeoKB. We may eventually get to the point where we kick off this process each time a new document and/or a new target scientific concept is organized into the GeoKB. It will be something automatic and dynamic where the knowledgebase is constantly (or maybe daily in practical terms) going out to work with AI and other assistive processes to build itself. In the near term, we're experimenting with the overall model and putting together building blocks.\n",
    "\n",
    "In the following codeblocks, I have a function that gives us a mapping between the identifiers we will find in our third party infrastructure (xDD) and the local QIDs in the GeoKB that we'll need to operate on. This takes a few seconds to complete because there are about 100K records, but then we have a full mapping to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geokb_id_map(pid, limit=10000):\n",
    "    id_lookup = {}\n",
    "    offset = 0\n",
    "\n",
    "    while True:\n",
    "        id_query = f\"\"\"\n",
    "        PREFIX wdt: <https://geokb.wikibase.cloud/prop/direct/>\n",
    "\n",
    "        SELECT ?item ?id\n",
    "        WHERE {{\n",
    "            ?item wdt:{pid} ?id .\n",
    "        }}\n",
    "        LIMIT {limit}\n",
    "        OFFSET {offset}\n",
    "        \"\"\"\n",
    "\n",
    "        id_items = geokb.sparql_query(\n",
    "            query=id_query,\n",
    "            endpoint=geokb.sparql_endpoint,\n",
    "            output=\"raw\"\n",
    "        )\n",
    "        if id_items is None:\n",
    "            break\n",
    "        else:\n",
    "            for x in id_items['results']['bindings']:\n",
    "                id_lookup[x['id']['value']] = x['item']['value'].split('/')[-1]\n",
    "            offset+=limit\n",
    "    \n",
    "    return id_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gddid_lookup = geokb_id_map('P93')\n",
    "doi_lookup = geokb_id_map('P74')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geoscientific Concepts\n",
    "\n",
    "I'm still experimenting with the best way to encode all of the different reference scientific concepts we want to work with in the GeoKB/Wikibase model. I'm trying to find a balance between rigorous semantics and practical use. In the current iteration, I've built out a subset of lithology terms and minerals (including varieties and groups) that we are working with as a test case. Some of these include \"same as\" linkages to two different Macrostrat references that serve as the basis for the xDD indexing (meaning we will turn them up in the terms or snippets API for some of our documents).\n",
    "\n",
    "In the following codeblock, I pull those items that have the macrostrat \"defs\" linkages and tee up the process that will hit the xDD API for results on those concepts. That gives me a data structure I can operate with to go after results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "macrostrat_def_same_as_query = \"\"\"\n",
    "  PREFIX wdt: <https://geokb.wikibase.cloud/prop/direct/>\n",
    "\n",
    "  SELECT ?item ?itemLabel ?same_as\n",
    "  WHERE {\n",
    "    ?item wdt:P84 ?same_as .\n",
    "    FILTER CONTAINS(STR(?same_as), \"macrostrat.org/api/defs\")\n",
    "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\" . }\n",
    "  }\n",
    "\"\"\"\n",
    "\n",
    "macrostrat_def_links = geokb.sparql_query(\n",
    "    query=macrostrat_def_same_as_query,\n",
    "    endpoint=geokb.sparql_endpoint,\n",
    "    output=\"dataframe\"\n",
    ")\n",
    "\n",
    "macrostrat_def_links['concept_qid'] = macrostrat_def_links['item'].apply(lambda x: x.split('/')[-1])\n",
    "macrostrat_def_links['dict_filter'] = macrostrat_def_links['same_as'].apply(lambda x: x.split('/')[5].split('?')[0])\n",
    "macrostrat_def_links['term'] = macrostrat_def_links['same_as'].apply(lambda x: x.split('/')[5].split('?')[-1].split('=')[-1])\n",
    "\n",
    "macrostrat_def_links['ni43101_snippets'] = macrostrat_def_links.apply(lambda x: f\"https://geodeepdive.org/api/snippets?dict_filter={x['dict_filter']}&publisher=Geoarchive&term={x['term']}\", axis=1)\n",
    "macrostrat_def_links['usgs_snippets'] = macrostrat_def_links.apply(lambda x: f\"https://geodeepdive.org/api/snippets?dict_filter={x['dict_filter']}&publisher=USGS&term={x['term']}\", axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve Results from xDD\n",
    "\n",
    "In this codeblock, I work through the API calls that will tell us what documents have which concepts represented. I'm using the snippets route with xDD here, though the terms route would probably suffice. I'm still fiddling with whether or not it is useful to incorporate the snippets of text where terms are found in some way. The most potentially useful approach I've experimented with is using the wiki page associated with the items representing documents in the Wikibase instance to write out snippets and any other details from any source we might want to incorporate. With those in place, both humans and machines will have additional semistructured but looseform fodder to work with in various ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "geokb_doc_subjects = []\n",
    "\n",
    "for index, row in macrostrat_def_links.iterrows():\n",
    "    r_ni43101 = requests.get(row['ni43101_snippets']).json()\n",
    "    if 'success' in r_ni43101 and r_ni43101['success']['data']:\n",
    "        for item in r_ni43101['success']['data']:\n",
    "            if item['_gddid'] in gddid_lookup:\n",
    "                geokb_doc_subjects.append((gddid_lookup[item['_gddid']],row['concept_qid'],row['ni43101_snippets']))\n",
    "\n",
    "    r_usgs_pubs = requests.get(row['usgs_snippets']).json()\n",
    "    if 'success' in r_usgs_pubs and r_usgs_pubs['success']['data']:\n",
    "        for item in r_usgs_pubs['success']['data']:\n",
    "            if 'doi' in item and item['doi'] in doi_lookup:\n",
    "                geokb_doc_subjects.append((doi_lookup[item['doi']],row['concept_qid'],row['usgs_snippets']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addresses Subject Claims\n",
    "The preceding step gives us everything we need to work with for committing dynamically generated content to the GeoKB. I wrote this as a list of sets containing the subject item (the document representation we will be adding claims to), the object item (geoscientific concept; lithology term or mineral species/variety/group in this case), and the specific API call used to derive the claim. The latter can be used as a reference for now, and it can be followed to see exactly how we derived the claim and leading a user to view the snippets to help evaluate whether it is worthwhile to write them into the Wikibase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Q40197',\n",
       "  'Q41757',\n",
       "  'https://geodeepdive.org/api/snippets?dict_filter=lithologies&publisher=Geoarchive&term=peridotite'),\n",
       " ('Q29971',\n",
       "  'Q41757',\n",
       "  'https://geodeepdive.org/api/snippets?dict_filter=lithologies&publisher=Geoarchive&term=peridotite'),\n",
       " ('Q34809',\n",
       "  'Q41757',\n",
       "  'https://geodeepdive.org/api/snippets?dict_filter=lithologies&publisher=Geoarchive&term=peridotite'),\n",
       " ('Q40195',\n",
       "  'Q41757',\n",
       "  'https://geodeepdive.org/api/snippets?dict_filter=lithologies&publisher=Geoarchive&term=peridotite'),\n",
       " ('Q34227',\n",
       "  'Q41757',\n",
       "  'https://geodeepdive.org/api/snippets?dict_filter=lithologies&publisher=Geoarchive&term=peridotite')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geokb_doc_subjects[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commit to GeoKB\n",
    "Once we have the claims data to work with, it's a matter of writing to the GeoKB. There are ultimately better ways of handling this at scale, but looping over a list grouped by the subject item we are adding to is reasonable enough at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q100365\n",
      "Q100665\n",
      "Q101510\n",
      "Q103030\n",
      "Q104901\n",
      "Q104989\n",
      "Q105506\n",
      "Q106867\n",
      "Q106938\n",
      "Q107170\n",
      "Q107699\n",
      "Q109951\n",
      "Q113086\n",
      "Q116634\n",
      "Q117883\n",
      "Q118029\n",
      "Q118539\n",
      "Q120001\n",
      "Q122136\n",
      "Q122278\n",
      "Q122502\n",
      "Q122519\n",
      "Q122846\n",
      "Q123577\n",
      "Q123709\n",
      "Q124501\n",
      "Q125370\n",
      "Q129085\n",
      "Q129339\n",
      "Q130088\n",
      "Q130301\n",
      "Q130608\n",
      "Q131503\n",
      "Q132548\n",
      "Q133289\n",
      "Q133292\n",
      "Q133931\n",
      "Q133953\n",
      "Q134006\n",
      "Q134019\n",
      "Q134100\n",
      "Q135313\n",
      "Q135357\n",
      "Q135545\n",
      "Q136023\n",
      "Q137627\n",
      "Q137675\n",
      "Q28938\n",
      "Q29186\n",
      "Q29259\n",
      "Q29716\n",
      "Q29891\n",
      "Q29971\n",
      "Q30581\n",
      "Q30742\n",
      "Q31241\n",
      "Q31577\n",
      "Q31737\n",
      "Q31802\n",
      "Q32272\n",
      "Q32279\n",
      "Q32512\n",
      "Q32562\n",
      "Q32591\n",
      "Q32640\n",
      "Q32956\n",
      "Q33378\n",
      "Q33459\n",
      "Q34042\n",
      "Q34227\n",
      "Q34374\n",
      "Q34558\n",
      "Q34809\n",
      "Q35011\n",
      "Q35501\n",
      "Q35583\n",
      "Q35803\n",
      "Q35807\n",
      "Q35815\n",
      "Q36100\n",
      "Q37033\n",
      "Q37106\n",
      "Q37163\n",
      "Q37400\n",
      "Q37599\n",
      "Q37860\n",
      "Q37869\n",
      "Q37901\n",
      "Q38505\n",
      "Q38658\n",
      "Q38759\n",
      "Q38774\n",
      "Q38974\n",
      "Q38987\n",
      "Q39045\n",
      "Q39077\n",
      "Q39307\n",
      "Q39343\n",
      "Q40195\n",
      "Q40197\n",
      "Q55177\n",
      "Q75832\n",
      "Q80287\n",
      "Q80898\n",
      "Q83551\n",
      "Q83591\n",
      "Q85481\n",
      "Q86183\n",
      "Q86560\n",
      "Q89163\n",
      "Q89244\n",
      "Q91418\n",
      "Q91805\n",
      "Q92631\n",
      "Q93042\n",
      "Q93233\n",
      "Q93240\n",
      "Q94435\n",
      "Q95131\n",
      "Q95440\n",
      "Q96127\n",
      "Q97603\n",
      "Q97899\n",
      "Q99754\n"
     ]
    }
   ],
   "source": [
    "for index, row in pd.DataFrame(geokb_doc_subjects, columns=['subject','object','reference']).groupby('subject', as_index=False)[['object','reference']].agg(list).iterrows():\n",
    "    item = geokb.wbi.item.get(row['subject'])\n",
    "    subject_claims = []\n",
    "    for i, subject_qid in enumerate(row['object']):\n",
    "        ref = geokb.models.References()\n",
    "        ref.add(\n",
    "            geokb.datatypes.URL(\n",
    "                prop_nr=geokb.prop_lookup['reference URL'],\n",
    "                value=row['reference'][i]\n",
    "            )\n",
    "        )\n",
    "        subject_claims.append(\n",
    "            geokb.datatypes.Item(\n",
    "                prop_nr=geokb.prop_lookup['addresses subject'],\n",
    "                value=subject_qid,\n",
    "                references=ref\n",
    "            )\n",
    "        )\n",
    "    item.claims.add(\n",
    "        subject_claims,\n",
    "        action_if_exists=geokb.action_if_exists.REPLACE_ALL\n",
    "    )\n",
    "    response = item.write(\n",
    "        summary=\"Added addresses subject claims pulled from xDD index\"\n",
    "    )\n",
    "    print(response.id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geokb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
