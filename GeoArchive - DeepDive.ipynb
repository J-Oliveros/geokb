{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook starts to work through the process of pulling value from the processing of documents we are working through with the xDD Cyberinfrastructure back into the representation of documents in the GeoKB. We will keep improving on this process through use and experimentation as we learn what is going to add the most value to assessments and other research activities.\n",
    "\n",
    "I'm starting with just the [period](https://geokb.wikibase.cloud/wiki/Special:WhatLinksHere/Item:Q26291) geochronological time concepts as those have an existing dedicated dictionary in xDD, it's a small list, and the terms are somewhat unique. We will likely want to winnow things down from this kind of crude process based solely on indexing hits across the corpus of documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import swifter\n",
    "from wbmaker import WikibaseConnection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "geokb = WikibaseConnection(\"GEOKB_CLOUD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_item_id = geokb.ref_lookup['xDD Cyberinfrastructure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_period = \"PREFIX%20wd%3A%20%3Chttps%3A%2F%2Fgeokb.wikibase.cloud%2Fentity%2F%3E%0APREFIX%20wdt%3A%20%3Chttps%3A%2F%2Fgeokb.wikibase.cloud%2Fprop%2Fdirect%2F%3E%0A%0ASELECT%20%3Fitem%20%3FitemLabel%0AWHERE%20%7B%0A%20%20%3Fitem%20wdt%3AP1%20wd%3AQ26291%20.%0A%20%20SERVICE%20wikibase%3Alabel%20%7B%20bd%3AserviceParam%20wikibase%3Alanguage%20%22en%22%20%7D%0A%7D%0A\"\n",
    "df_period = geokb.wb_ref_data(query=query_period)\n",
    "df_period[\"item\"] = df_period.item.apply(lambda x: x.split(\"/\")[-1])\n",
    "\n",
    "query_gddid = \"PREFIX%20wdt%3A%20%3Chttps%3A%2F%2Fgeokb.wikibase.cloud%2Fprop%2Fdirect%2F%3E%0A%0ASELECT%20%3Fitem%20%3Fgddid%0AWHERE%20%7B%0A%20%20%3Fitem%20wdt%3AP93%20%3Fgddid%20.%0A%20%20SERVICE%20wikibase%3Alabel%20%7B%20bd%3AserviceParam%20wikibase%3Alanguage%20%22en%22%20%7D%0A%7D%0A\"\n",
    "df_gddid = geokb.wb_ref_data(query=query_gddid)\n",
    "df_gddid[\"item\"] = df_gddid.item.apply(lambda x: x.split(\"/\")[-1])\n",
    "gddid_lookup = df_gddid.set_index('gddid')['item'].to_dict()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xDD Interface\n",
    "\n",
    "I need to keep working through the best way to interface with the xDD APIs on this type of process. The following is just my first attempt. There are likely more efficient ways to get this done. This process comes at it from the standpoint of a known linkage - a \"period\" as a subdivision of geologic time. We previously set up this reference source in the GeoKB by processing the International Geochronostratigraphic Chart, which is essentially the same source that xDD is using across several different dictionaries. As uniquely named entities within context, this provides us reasonable assurity that we are dealing with the same concepts.\n",
    "\n",
    "I initially tried using the terms API from xDD, sending a series of requests for the terms we know about in the GeoKB on the corpus of documents for which we have representation in the GeoKB. Given the identifiers we have in common, this provides everything necessary to post a set of claims on NI 43-101 reports, linking them to period entities via the broad \"addresses subject\" predicate.\n",
    "\n",
    "However, once I worked that through and thought about how we might want to interact with these claims, I ended up using the snippets API and pulling back highlights to incorporate into the item claims as qualifiers. This may or may not prove the most useful, but we can always revisit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_records = []\n",
    "for index, row in df_period.iterrows():\n",
    "    subject_qid = row[\"item\"]\n",
    "    subject = f\"{row.itemLabel},{row.itemLabel.replace(' Period','').strip()}\"\n",
    "\n",
    "    next_page = f\"https://geodeepdive.org/api/snippets?publisher=Geoarchive&term={subject}&dictid_filter=71&full_results&clean\"\n",
    "    while next_page:\n",
    "        response = requests.get(next_page).json()\n",
    "        if \"success\" in response:\n",
    "            all_records.extend([dict(item, **{'subject_qid': subject_qid, 'subject': subject}) for item in response[\"success\"][\"data\"]])\n",
    "            if \"next_page\" in response[\"success\"] and response[\"success\"][\"next_page\"]:\n",
    "                next_page = response[\"success\"][\"next_page\"]\n",
    "            else:\n",
    "                next_page = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdd_ref = geokb.datatypes.Item(\n",
    "    prop_nr=geokb.prop_lookup['knowledge source'],\n",
    "    value=source_item_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gddids = list(set([i[\"_gddid\"] for i in all_records]))\n",
    "\n",
    "for gddid in gddids:\n",
    "    item_qid = gddid_lookup[gddid]\n",
    "    item = geokb.wbi.item.get(item_qid)\n",
    "\n",
    "    item_records = [i for i in all_records if i[\"_gddid\"] == gddid]\n",
    "\n",
    "    claims = []\n",
    "    for x in item_records:\n",
    "        highlight_qualifiers = geokb.models.Qualifiers()\n",
    "        for snippet in x[\"highlight\"]:\n",
    "            highlight_qualifiers.add(\n",
    "                geokb.datatypes.String(\n",
    "                    prop_nr=geokb.prop_lookup['concept highlight'],\n",
    "                    value=snippet\n",
    "                )\n",
    "            )\n",
    "\n",
    "        snippet_ref = geokb.datatypes.URL(\n",
    "            prop_nr=geokb.prop_lookup['reference URL'],\n",
    "            value=f\"https://geodeepdive.org/api/snippets?docid={x['_gddid']}&term={x['subject'].replace(' ','%20')}&dictid_filter=71&full_results&clean\"\n",
    "        )\n",
    "\n",
    "        refs = geokb.models.References()\n",
    "        refs.add(xdd_ref)\n",
    "        refs.add(snippet_ref)\n",
    "        \n",
    "        claims.append(\n",
    "            geokb.datatypes.Item(\n",
    "                prop_nr=geokb.prop_lookup['addresses subject'],\n",
    "                value=x[\"subject_qid\"],\n",
    "                references=refs,\n",
    "                qualifiers=highlight_qualifiers\n",
    "            )\n",
    "        )\n",
    "\n",
    "    item.claims.add(claims)\n",
    "    try:\n",
    "        item.write(\n",
    "            summary=\"Added claims for geochronologic period based on xDD snippet extraction\",\n",
    "            clear=True\n",
    "        )\n",
    "    except:\n",
    "        pass\n",
    "    print(item_qid)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geokb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
