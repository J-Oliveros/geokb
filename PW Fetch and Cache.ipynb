{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 177K+ records in the Pubs Warehouse catalog, we want to establish a baseline of representation for these in the GeoKB and then run regular updates for new and refreshed metadata. This notebook has two parts:\n",
    "* Cache raw data as pickle files via a loop against the PW web service\n",
    "* Pre-process raw data into an efficient structure we can work from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve raw metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import requests\n",
    "import pickle\n",
    "\n",
    "page_num = 1\n",
    "num_records = 0\n",
    "\n",
    "while True:\n",
    "    api = f\"https://pubs.er.usgs.gov/pubs-services/publication/?page_size=1000&page_number={page_num}\"\n",
    "    pw = requests.get(api)\n",
    "    \n",
    "    if pw.status_code != 200:\n",
    "        break\n",
    "\n",
    "    pw_records = pw.json()\n",
    "    \n",
    "    if pw_records[\"records\"]:\n",
    "        pickle.dump(pw_records[\"records\"], open(f\"./data/pwdump/page_{page_num}.pickle\", \"wb\"))\n",
    "        num_records+=len(pw_records[\"records\"])\n",
    "        page_num += 1\n",
    "        print(\n",
    "            \"PAGE NUMBER:\", \n",
    "            pw_records['pageNumber'], \n",
    "            \"| PAGE ROW START:\", \n",
    "            pw_records['pageRowStart'],\n",
    "            \"| RECORDS CACHED:\", \n",
    "            num_records,\n",
    "            \"| REMAINING RECORDS:\", \n",
    "            int(pw_records['recordCount']) - num_records\n",
    "        )\n",
    "    else:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process PW Catalog\n",
    "\n",
    "There are a number of ways that the PW Catalog has packaged information that add additional stuff we can't do anything with (e.g., internal identifiers) or are otherwise difficult to deal with. Taking the raw metadata that we dumped to a collection of pickle files, we can pull out the parts we can use in building a knowledge representation, run a couple of validation steps, and build a minimal transformation that includes the following:\n",
    "* Core text string fields that have some utility in building the representation in the GeoKB\n",
    "* Collection of links that provide some clues on underlying content\n",
    "* Contributors that have ORCIDs (the only ones we are processing at this time)\n",
    "* Valid GeoJSON feature collections that we may use in future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import json\n",
    "import geojson\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"./data/pwdump\"\n",
    "\n",
    "all_dicts = []\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".pickle\"):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        with open(file_path, 'rb') as file:\n",
    "            # Load the list of dictionaries from each pickle file\n",
    "            data = pickle.load(file)\n",
    "            all_dicts.extend(data)\n",
    "\n",
    "pw_dump = pd.DataFrame(all_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pw_core_props = [\n",
    "    \"indexId\",\n",
    "    \"doi\",\n",
    "    \"publisher\",\n",
    "    \"lastModifiedDate\",\n",
    "    \"displayToPublicDate\",\n",
    "    \"publishedDate\",\n",
    "    \"revisedDate\",\n",
    "    \"publicationYear\",\n",
    "    \"title\",\n",
    "    \"docAbstract\",\n",
    "    \"tableOfContents\",\n",
    "    \"usgsCitation\",\n",
    "    \"country\",\n",
    "    \"state\",\n",
    "    \"county\",\n",
    "    \"city\",\n",
    "    \"otherGeospatial\",\n",
    "    \"ipdsId\",\n",
    "    \"pub_type\",\n",
    "    \"pub_subtype\",\n",
    "    \"series_title\",\n",
    "    \"part_of\",\n",
    "    \"superseded_by\",\n",
    "    \"cost_centers\",\n",
    "    \"programNote\",\n",
    "    \"pub_rel\",\n",
    "    \"numberOfPages\"\n",
    "]\n",
    "\n",
    "pw_dump['pub_type'] = pw_dump['publicationType'].apply(lambda x: x['text'] if isinstance(x, dict) else None)\n",
    "pw_dump['pub_subtype'] = pw_dump['publicationSubtype'].apply(lambda x: x['text'] if isinstance(x, dict) else None)\n",
    "pw_dump['series_title'] = pw_dump['seriesTitle'].apply(lambda x: x['text'] if isinstance(x, dict) else None)\n",
    "pw_dump['part_of'] = pw_dump['isPartOf'].apply(lambda x: x['indexId'] if isinstance(x, dict) else None)\n",
    "pw_dump['superseded_by'] = pw_dump['supersededBy'].apply(lambda x: x['indexId'] if isinstance(x, dict) else None)\n",
    "\n",
    "pw_dump['cost_centers'] = pw_dump['costCenters'].apply(lambda x: [i['text'] for i in x])\n",
    "\n",
    "pw_dump['pub_rel'] = pw_dump['interactions'].apply(lambda x: [':'.join([i['subject']['indexId'], i['predicate'], i['object']['indexId']]) for i in x])\n",
    "\n",
    "pw_core = pw_dump[pw_core_props].reset_index(drop=True)\n",
    "\n",
    "pw_core.to_parquet('./data/pw_cache/pw_core.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pw_links = pw_dump[pw_dump['links'].notnull()][['indexId','links']].reset_index(drop=True).explode('links')\n",
    "pw_links['link_type'] = pw_links['links'].apply(lambda x: x['type']['text'])\n",
    "pw_links['link_url'] = pw_links['links'].apply(lambda x: x['url'])\n",
    "pw_links.drop(columns=\"links\", inplace=True)\n",
    "\n",
    "pw_links.to_parquet('./data/pw_cache/pw_links.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'authors': [{'text': 'Lee, Willis T.',\n",
       "   'contributorId': 87524,\n",
       "   'corporation': False,\n",
       "   'usgs': True,\n",
       "   'family': 'Lee',\n",
       "   'given': 'Willis T.',\n",
       "   'affiliations': [],\n",
       "   'preferred': False,\n",
       "   'id': 221485,\n",
       "   'contributorType': {'id': 1, 'text': 'Authors'},\n",
       "   'rank': 1}]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pw_dump[pw_dump['contributors'].notnull()][['indexId','contributors']].iloc[0]['contributors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_contributors(contributors):\n",
    "    pub_contributors = []\n",
    "    for role, contrib_list in contributors.items():\n",
    "        for i in contrib_list:\n",
    "            if not i[\"corporation\"] and \"orcid\" in i and i[\"orcid\"].startswith('https://orcid.org/'):\n",
    "                affiliations = None\n",
    "                if \"affiliations\" in i and i[\"affiliations\"]:\n",
    "                    affiliations = [x[\"text\"] for x in i[\"affiliations\"] if x[\"usgs\"]]\n",
    "                pub_contributors.append({\n",
    "                    \"orcid\": i[\"orcid\"].split(\"/\")[-1],\n",
    "                    \"usgs\": i[\"usgs\"],\n",
    "                    \"pub_role\": role,\n",
    "                    \"usgs_affiliations\": affiliations\n",
    "                })\n",
    "\n",
    "    return pub_contributors\n",
    "\n",
    "pw_contributors = pw_dump[pw_dump['contributors'].notnull()][['indexId','contributors']].reset_index(drop=True)\n",
    "pw_contributors['orcid_contributors'] = pw_contributors['contributors'].apply(parse_contributors)\n",
    "pw_contributors.drop(columns=\"contributors\", inplace=True)\n",
    "pw_contributors = pw_contributors[pw_contributors['orcid_contributors'].str.len() > 0]\n",
    "\n",
    "pw_contributors = pw_contributors.explode('orcid_contributors').reset_index(drop=True)\n",
    "\n",
    "pw_contributors = pd.concat([\n",
    "    pw_contributors.drop('orcid_contributors', axis=1),\n",
    "    pw_contributors['orcid_contributors'].apply(pd.Series)\n",
    "], axis=1)\n",
    "\n",
    "pw_contributors.to_parquet('./data/pw_cache/pw_contributors.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pw_contributors_raw = pw_dump[pw_dump['contributors'].notnull()][['indexId','contributors']].reset_index(drop=True)\n",
    "\n",
    "pw_contributors_all = pd.concat([pw_contributors_raw.drop(columns=\"contributors\"), pd.json_normalize(pw_contributors_raw['contributors'])], axis=1)\n",
    "\n",
    "pw_authors = pw_contributors_all[['indexId','authors']].dropna().reset_index(drop=True)\n",
    "pw_authors = pw_authors.explode(\"authors\").reset_index(drop=True)\n",
    "pw_authors = pd.concat([pw_authors.drop(columns=\"authors\"), pd.json_normalize(pw_authors['authors'])], axis=1)\n",
    "pw_authors['creatorType'] = \"author\"\n",
    "pw_authors = pw_authors[['indexId','creatorType','given','family']].dropna().rename(columns={'given': 'firstName', 'family': 'lastName'})\n",
    "\n",
    "pw_editors = pw_contributors_all[['indexId','editors']].dropna().reset_index(drop=True)\n",
    "pw_editors = pw_editors.explode(\"editors\").reset_index(drop=True)\n",
    "pw_editors = pd.concat([pw_editors.drop(columns=\"editors\"), pd.json_normalize(pw_editors['editors'])], axis=1)\n",
    "pw_editors['creatorType'] = \"editor\"\n",
    "pw_editors = pw_editors[['indexId','creatorType','given','family']].dropna().rename(columns={'given': 'firstName', 'family': 'lastName'})\n",
    "\n",
    "pw_compilers = pw_contributors_all[['indexId','compilers']].dropna().reset_index(drop=True)\n",
    "pw_compilers = pw_compilers.explode(\"compilers\").reset_index(drop=True)\n",
    "pw_compilers = pd.concat([pw_compilers.drop(columns=\"compilers\"), pd.json_normalize(pw_compilers['compilers'])], axis=1)\n",
    "pw_compilers['creatorType'] = \"compiler\"\n",
    "pw_compilers = pw_compilers[['indexId','creatorType','given','family']].dropna().rename(columns={'given': 'firstName', 'family': 'lastName'})\n",
    "\n",
    "pd.concat([pw_authors, pw_editors, pw_compilers]).reset_index(drop=True).to_parquet('./data/pw_cache/pw_creators.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_geojson(geojson_string):\n",
    "    check_obj = geojson.loads(geojson_string)\n",
    "    if check_obj.is_valid:\n",
    "        return geojson.dumps(check_obj)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "pw_geo = pw_dump[pw_dump['geographicExtents'].notnull()][['indexId','geographicExtents']].reset_index(drop=True)\n",
    "pw_geo['geojson'] = pw_geo['geographicExtents'].apply(json.loads)\n",
    "pw_geo.drop(columns=\"geographicExtents\", inplace=True)\n",
    "pw_geo['geojson'] = pw_geo['geojson'].apply(json.dumps)\n",
    "pw_geo['geojson'] = pw_geo['geojson'].apply(check_geojson)\n",
    "\n",
    "pw_geo.dropna(subset=\"geojson\", inplace=True)\n",
    "\n",
    "pw_geo.to_parquet('./data/pw_cache/pw_geo.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default *",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
